# Project: Prompt Chaining with Multiple LLMs in Rust! | Multi-Model Prompt Flow

## Stack

- Rust
- Multiple LLMs
- Prompt Chaining

## Links

## Helpful commands

```bash
cargo run --quiet
```

```bash
ollama ps
```

```bash
ollama ls
```

```bash
ollama pull gemma3:1b
```

```bash
ollama run gemma3:1b
```

```bash
ollama pull llama3.2:1b
```

```bash
ollama run llama3.2:1b
```
